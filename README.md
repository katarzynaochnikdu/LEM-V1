# System Oceny Kompetencji LEM - MVP

> **Automatyczny, audytowalny system oceny kompetencji menedÅ¼erskich z wykorzystaniem AI**

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115.0-green.svg)](https://fastapi.tiangolo.com/)
[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4o-orange.svg)](https://openai.com/)

---

## ðŸŽ¯ Czym jest ten system?

System automatycznie ocenia odpowiedzi narracyjne uczestnikÃ³w assessmentu menedÅ¼erskiego w zakresie kompetencji **Delegowanie** wedÅ‚ug modelu LEM. 

**Kluczowe cechy**:
- âœ… Ocena 0-4 (co 0.25 punktu) oparta na eksperckiej rubryce 7 wymiarÃ³w
- âœ… Ekstrakcja cytatÃ³w-dowodÃ³w z odpowiedzi (peÅ‚na audytowalnoÅ›Ä‡)
- âœ… Spersonalizowany feedback rozwojowy
- âœ… Transparentny breakdown oceny na wymiary
- âœ… API gotowe do integracji

---

## ðŸš€ Szybki start (5 minut)

### 1. Instalacja

```bash
# Sklonuj projekt
cd c:\Users\kochn\.cursor\Daniel\LEM

# Zainstaluj zaleÅ¼noÅ›ci
pip install -r requirements.txt

# Skonfiguruj API key
copy .env.example .env
# Edytuj .env i dodaj: OPENAI_API_KEY=sk-...
```

### 2. Uruchomienie

```bash
# Uruchom serwer
uvicorn app.main:app --reload

# OtwÃ³rz dokumentacjÄ™ API
# http://localhost:8000/docs
```

### 3. Test

```bash
# Uruchom test na przykÅ‚adowych odpowiedziach
python tests/run_manual_test.py
```

**SzczegÃ³Å‚owa instrukcja**: Patrz [`QUICKSTART.md`](QUICKSTART.md) lub [`INSTALLATION.md`](INSTALLATION.md)

---

## ðŸ“š Dokumentacja

| Dokument | Opis | Czas czytania |
|----------|------|---------------|
| **[QUICKSTART.md](QUICKSTART.md)** | Szybki start - pierwsze uruchomienie | 5 min |
| **[INSTALLATION.md](INSTALLATION.md)** | SzczegÃ³Å‚owa instrukcja instalacji | 10 min |
| **[ARCHITECTURE.md](ARCHITECTURE.md)** | Architektura techniczna systemu | 15 min |
| **[CALIBRATION_GUIDE.md](CALIBRATION_GUIDE.md)** | Instrukcja kalibracji z asesorami | 20 min |
| **[PROJECT_SUMMARY.md](PROJECT_SUMMARY.md)** | Podsumowanie projektu | 10 min |

---

## ðŸ— Architektura

System skÅ‚ada siÄ™ z **4 moduÅ‚Ã³w** przetwarzania sekwencyjnego:

```
OdpowiedÅº â†’ [Parser] â†’ [Mapper] â†’ [Scorer] â†’ [Feedback] â†’ Ocena + Feedback
            â†“           â†“           â†“           â†“
         Strukturyzacja Ekstrakcja  Algorytm   Personalizacja
         na sekcje      cytatÃ³w     oceny      feedbacku
```

### ModuÅ‚y

1. **Parser** (`app/modules/parser.py`) - Strukturyzacja odpowiedzi na 4 sekcje logiczne
2. **Mapper** (`app/modules/mapper.py`) - Ekstrakcja cytatÃ³w dla 7 wymiarÃ³w delegowania
3. **Scorer** (`app/modules/scorer.py`) - Algorytm oceny 0-4 z wagami wymiarÃ³w
4. **Feedback Generator** (`app/modules/feedback.py`) - Generowanie spersonalizowanego feedbacku

**SzczegÃ³Å‚y**: Patrz [`ARCHITECTURE.md`](ARCHITECTURE.md)

---

## ðŸ“Š 7 WymiarÃ³w Delegowania

System ocenia odpowiedzi wedÅ‚ug 7 wymiarÃ³w kompetencji Delegowanie:

| Wymiar | Waga | Opis |
|--------|------|------|
| **Intencja** | 10% | Nadawanie sensu biznesowego zadaniu |
| **Stan docelowy** | 20% | Precyzja opisu rezultatu koÅ„cowego |
| **Metoda pomiaru** | 15% | WskaÅºniki/produkty/zachowania |
| **Poziom odpowiedzialnoÅ›ci** | 20% | Delegowanie odpowiedzialnoÅ›ci, nie tylko zadaÅ„ |
| **Harmonogram** | 10% | Konsultacja terminÃ³w z pracownikiem |
| **Monitorowanie** | 10% | Plan kontroli przebiegu |
| **Sprawdzenie zrozumienia** | 15% | Pytania otwarte o rozumienie |

**Rubryka szczegÃ³Å‚owa**: Patrz [`app/rubric.py`](app/rubric.py)

---

## ðŸ”Œ API Endpoints

### `POST /assess` - GÅ‚Ã³wny endpoint oceny

**Request**:
```json
{
  "participant_id": "P001",
  "response_text": "PrzygotowujÄ…c siÄ™ do rozmowy delegujÄ…cej...",
  "case_id": "delegowanie_bnp_v1"
}
```

**Response**:
```json
{
  "participant_id": "P001",
  "timestamp": "2026-02-22T10:30:00Z",
  "competency": "delegowanie",
  "score": 2.75,
  "level": "Efektywny (Åšwiadoma kompetencja)",
  "evidence": {
    "intencja": ["WyjaÅ›niam kontekst biznesowy..."],
    "stan_docelowy": ["Proces ma byÄ‡ gotowy do 31 marca..."]
  },
  "feedback": {
    "summary": "Precyzyjnie definiujesz stan docelowy...",
    "recommendation": "Wzmocnij delegowanie odpowiedzialnoÅ›ci...",
    "mocne_strony": [...],
    "obszary_rozwoju": [...]
  },
  "dimension_scores": {
    "intencja": 0.8,
    "stan_docelowy": 0.9,
    ...
  }
}
```

### Inne endpointy

- `GET /health` - Health check
- `GET /dimensions` - Definicje wymiarÃ³w
- `GET /weights` - Aktualne wagi wymiarÃ³w

**Dokumentacja interaktywna**: http://localhost:8000/docs (gdy serwer dziaÅ‚a)

---

## ðŸ§ª Testy

### Testy jednostkowe

```bash
# Wszystkie testy
pytest tests/ -v

# Tylko Parser
pytest tests/test_parser.py -v

# Tylko Scorer
pytest tests/test_scorer.py -v

# Testy integracyjne (end-to-end)
pytest tests/test_integration.py -v -s
```

### Test manualny

```bash
# Przetestuj na 5 przykÅ‚adowych odpowiedziach
python tests/run_manual_test.py
```

**Dane testowe**: 5 syntetycznych odpowiedzi w `tests/sample_responses/`

---

## ðŸ“‚ Struktura projektu

```
lem-assessment/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                    # FastAPI server
â”‚   â”œâ”€â”€ models.py                  # Pydantic models (8 modeli)
â”‚   â”œâ”€â”€ rubric.py                  # Rubryka Delegowanie (7 wymiarÃ³w)
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ parser.py              # ModuÅ‚ 1: Strukturyzacja
â”‚   â”‚   â”œâ”€â”€ mapper.py              # ModuÅ‚ 2: Ekstrakcja dowodÃ³w
â”‚   â”‚   â”œâ”€â”€ scorer.py              # ModuÅ‚ 3: Algorytm oceny
â”‚   â”‚   â””â”€â”€ feedback.py            # ModuÅ‚ 4: Generator feedbacku
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ parse_prompt.txt       # Prompt Parsera
â”‚       â”œâ”€â”€ map_prompt.txt         # Prompt Mappera
â”‚       â””â”€â”€ feedback_prompt.txt    # Prompt Feedbacku
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_*.py                  # Testy jednostkowe
â”‚   â”œâ”€â”€ run_manual_test.py         # Skrypt manualnego testu
â”‚   â””â”€â”€ sample_responses/          # 5 przykÅ‚adowych odpowiedzi
â”œâ”€â”€ calibration/
â”‚   â”œâ”€â”€ run_calibration.py         # Skrypt kalibracji
â”‚   â”œâ”€â”€ analyze_results.py         # Analiza wynikÃ³w
â”‚   â””â”€â”€ README.md                  # Instrukcja kalibracji
â”œâ”€â”€ config/
â”‚   â””â”€â”€ weights.json               # Wagi wymiarÃ³w (kalibrowane)
â”œâ”€â”€ requirements.txt               # ZaleÅ¼noÅ›ci Python
â”œâ”€â”€ .env.example                   # Template zmiennych Å›rodowiskowych
â””â”€â”€ *.md                           # Dokumentacja
```

**Statystyki**: 34 pliki, 11 moduÅ‚Ã³w Python, 5 dokumentÃ³w

---

## ðŸŽ¯ Kalibracja

System wymaga kalibracji z ocenami asesorÃ³w przed wdroÅ¼eniem produkcyjnym.

**Proces kalibracji** (2-3 tygodnie):
1. Zebranie 20-30 prawdziwych odpowiedzi
2. Ocena przez 2-3 niezaleÅ¼nych asesorÃ³w
3. Uruchomienie systemu AI na tych samych danych
4. Analiza zgodnoÅ›ci (cel: korelacja >0.85, MAE <0.5)
5. Dostosowanie wag w `config/weights.json`
6. Walidacja krzyÅ¼owa

**SzczegÃ³Å‚owa instrukcja**: Patrz [`CALIBRATION_GUIDE.md`](CALIBRATION_GUIDE.md)

---

## ðŸ’° Koszty

### Koszty operacyjne (produkcja)

- **Na 1 ocenÄ™**: ~$0.10-0.15 (GPT-4o)
- **Na 100 uczestnikÃ³w**: ~$12
- **Na 1000 uczestnikÃ³w/rok**: ~$120/rok

### Optymalizacje (przyszÅ‚oÅ›Ä‡)

- Fine-tuning: -30% kosztÃ³w
- Caching: -40% kosztÃ³w
- Hybrid approach: -60% kosztÃ³w

---

## ðŸ”§ Stack technologiczny

| Warstwa | Technologia | Wersja |
|---------|-------------|--------|
| Backend | FastAPI | 0.115.0 |
| Runtime | Python | 3.11+ |
| LLM | OpenAI GPT-4o | latest |
| Validation | Pydantic | 2.9.2 |
| Testing | pytest | 8.3.3 |
| HTTP | uvicorn + httpx | latest |

---

## ðŸ“ˆ Roadmap

### âœ… MVP (ukoÅ„czone)

- [x] 4 moduÅ‚y przetwarzania (Parser, Mapper, Scorer, Feedback)
- [x] API FastAPI z 3 endpoints
- [x] Rubryka 7 wymiarÃ³w Delegowanie
- [x] Dane testowe (5 przykÅ‚adowych odpowiedzi)
- [x] Testy jednostkowe i integracyjne
- [x] NarzÄ™dzia kalibracji
- [x] Dokumentacja

### ðŸ”„ NastÄ™pne kroki

1. **Kalibracja** (2-3 tygodnie) - Dostosowanie wag z asesorami
2. **Walidacja** (1 tydzieÅ„) - Potwierdzenie stabilnoÅ›ci
3. **Skalowanie** (2-3 tygodnie) - Dodanie 3 pozostaÅ‚ych kompetencji LEM
4. **Integracja** (1-2 tygodnie) - PoÅ‚Ä…czenie z platformÄ… testowÄ…

---

## âš ï¸ Znane ograniczenia MVP

- âŒ Brak kalibracji - wagi wymiarÃ³w sÄ… szacunkowe
- âŒ Jedna kompetencja - tylko Delegowanie (3 pozostaÅ‚e do dodania)
- âŒ Brak cache'owania - kaÅ¼de wywoÅ‚anie to peÅ‚ne przetwarzanie
- âŒ Brak rate limiting - API nie ma ograniczeÅ„ requestÃ³w
- âŒ Brak persystencji - wyniki nie sÄ… zapisywane
- âŒ Brak auth - API jest otwarte

---

## ðŸ¤ WkÅ‚ad i rozwÃ³j

### Rozszerzenie na 4 kompetencje

System jest zaprojektowany do Å‚atwego rozszerzenia na pozostaÅ‚e 3 kompetencje LEM:
- Podejmowanie decyzji na bazie kryteriÃ³w
- OkreÅ›lanie celÃ³w i priorytetÃ³w
- Udzielanie informacji zwrotnej

**Szacunek pracy**: ~40% dodatkowej pracy (gÅ‚Ã³wnie rubryki + prompty)

---

## ðŸ“ž Wsparcie

### Dokumentacja

- **Quick Start**: [`QUICKSTART.md`](QUICKSTART.md)
- **Instalacja**: [`INSTALLATION.md`](INSTALLATION.md)
- **Architektura**: [`ARCHITECTURE.md`](ARCHITECTURE.md)
- **Kalibracja**: [`CALIBRATION_GUIDE.md`](CALIBRATION_GUIDE.md)
- **Podsumowanie**: [`PROJECT_SUMMARY.md`](PROJECT_SUMMARY.md)

### API Docs

- Swagger UI: http://localhost:8000/docs (gdy serwer dziaÅ‚a)

### Troubleshooting

Patrz sekcja "Troubleshooting" w [`INSTALLATION.md`](INSTALLATION.md)

---

## ðŸ“„ Licencja

Proprietary - BNP Paribas

---

## âœ¨ Podsumowanie

System Oceny LEM MVP jest **w peÅ‚ni funkcjonalny** i gotowy do:

âœ… Testowania na prawdziwych odpowiedziach  
âœ… Kalibracji z ocenami asesorÃ³w  
âœ… Walidacji przez HR i compliance  
âœ… Rozszerzenia na 4 kompetencje  

**Kluczowe osiÄ…gniÄ™cie**: Transparentny, audytowalny system AI ktÃ³ry **wspiera** (nie zastÄ™puje) ekspertÃ³w w ocenie kompetencji menedÅ¼erskich.

---

**Projekt ukoÅ„czony! ðŸŽ‰**  
**Wersja**: 1.0.0  
**Data**: 22 lutego 2026
